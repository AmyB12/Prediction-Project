Data Analysis And Machine Learning on Campus Placement

Exploratory Data Analysis

    -This is a prediction of whether students will get placed or not "two opt therefore a Binary Classification"
    -Need to determine the characteristics that can affect getting a placement
    - Need to determine the characteristics that can affect the students' salary
    -Make a prediction of the salary that can be secured by students "Regression - a measure of the relation between
     the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost)."

Going to answer some common questions

    -Does GPA affect placement?
    -Does Higher Secondary School's Percentage still affect campus placement?
    -Is work experience required for securing good job?
    -What factor affect the salary?

Steps:

Import libraries (importing pandas, numpy, mathplotlib, seaborn and pandas_profiling
Load data
Exploratory Data Analysis (attempted to export in different methods to see the outcome)
-exported in html> right-click the html> open in> browser> default
--Information from report
-missing 67 values in salary (need NaN)
-data does scale(salary range 200-900, rest of numerical data in percentages
-lots of outliers in high salary end
Exploring Data by each feature.
Feature gender *print*
M    139
F     76
Name: gender, dtype: int64

Describe the data- shows the statistical info on the data (e.g. mean, std, and percentiles)
            ssc_p       hsc_p  ...       mba_p         salary
count  215.000000  215.000000  ...  215.000000     148.000000
mean    67.303395   66.333163  ...   62.278186  288655.405405
std     10.827205   10.897509  ...    5.833385   93457.452420
min     40.890000   37.000000  ...   51.210000  200000.000000
25%     60.600000   60.900000  ...   57.945000  240000.000000
50%     67.000000   65.000000  ...   62.000000  265000.000000
75%     75.700000   73.000000  ...   66.255000  300000.000000
max     89.400000   97.700000  ...   77.890000  940000.000000

Find which columns are missing values.
gender             0
ssc_p              0  -secondary education percentage
ssc_b              0  -board of education (secondary)
hsc_p              0  -high school education percentage
hsc_b              0  -board of education (high)
hsc_s              0  -specialization in high school
degree_p           0  -degree percentage
degree_t           0  -under grad field
workex             0  -work experience
etest_p            0  -employability test percentage
specialisation     0  -post grad specialization
mba_p              0  -mba percentage
status             0
salary            67

There were issues with the matplotlib.
The Error:
UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure
Temporary Fix:
matplotlib.use('TkAgg') changing the backend GUI from agg(original) to TkAgg(from Tinker)
sns.pairplot(data.drop("salary", axis=1), hue="status")
plt.show()

kdeplot looks different from example

My Insights
    -There is 139 Males and 76 Females in this dataset
    -There is higher placements for males than females (countplot)
    -There were more outliers in males (salary) (kdeplot)
    -My results are a bit different but its showing that females are offered a higher salary than males

-Does secondary education affect placement? based on kdeplot students with averages below 50% weren't placed
and students with a higher percentages were able to be placed.
-The board does not seem to have much of an effect on the placement status.
-There are outliers in both but students from Central board are paid more from boxplot. (main about seems close)
-From the lineplot there isn't any real correlations between salary and the secondary percentage.
-To me there seems to be a higher salary for those in the central board but there isn't any real effect.
-Does higher secondary education affect placement? based on kdeplot there is an overlap, but it still shows that
students with a higher grade gets placed more than those with lower percentages.
-Based on countplot there seems to not be any effect on the placement status by the high sec board.
-Based on countplot there were fewer students that specialise in arts more in science and commerce, from the
students that did science and commerce there are more students placed (~2:1).
-Based on the boxplot there was not much difference in the effect of the board on the students' salaries
but the highest paid student was from central as seen from the secondary ed board.
-Based on the lineplot there seems to be high salaries in both boards therefore no real correlation, in the percentages
as well there is no real correlation with the salary as well. Therefore, there is no effect from both features on the salary.
-Based on the boxplot there seems to be a higher salary in the students that studied commerce than the other courses.
However, there is too little a sample size from the Art course as seen in previous count plot.
-Based on the lineplot the art students are plaid lower compared to the others even with moderately high percentages
there seems to not be much relation between the grades and salary of the other courses and between the courses(except art)
and salary.
-Does under grad affect placement? based on kdeplot there is again an overlap, but with higher grades there were more
placements but there are is still students with 50% that still get placed.
-The sample set from the students in other (based on count plot) was small to make a decision but in the science and commerce
seems to have roughly a 2:1 ratio as seen previously.
-Based on the boxplot there were more students on average with a high salary, but the commerce students have the highest paid job.
-Based on lineplot there seems to be no effect from the percentages on the salary, the highest plaid job is seen in the commerce.
-Does work experience affect placements? from the count plot there is an effect of the work experience on the placement
there are only a few students with work experience that was NOT placed.
-Students with work experience seem to be able to get a higher paid role than those who didn't(higher outlier).
-Based on the kdeplot (high overlap)there doesn't seem to be any effect of the employability test percentage on the placement.
-Based on lineplot there was no effect from the employability test percentage on the salary (surprisingly).
-Based on the count plot the specialisation does affect the placement, there were fewer students that are NOT placed while
doing MKTandFIN.
-Based on boxplots students in MKTandFIN have the higher paying jobs.
-Does MBA percentage affect placements? based on the boxplot mba percentages students with higher percentages were placed(slightly).
-------I guess it's not that effective
-Based on the lineplot the mba percentage does not affect the salary of the students.

Feature Selection
Using Only following features
(Ignoring Board of Education -> there wasn't much effect)

-Gender
-Secondary Education Percentage
-Higher Secondary Education Percentage
-Specialization in Higher Secondary Education
-Undergraduate Degree Percentage
-Undergraduate Degree Field
-Work Experience
-Employability Test Percentage
-Specialization
-MBA Percentage

These features seem to have some effect on the Status of the student (even if little).

The data types of the different columns
gender             object
ssc_p             float64
hsc_p             float64
hsc_s              object
degree_p          float64
degree_t           object
workex             object
etest_p           float64
specialisation     object
mba_p             float64
status             object
salary            float64

Predicting if students get placed or not (binary classification problem) - comparing 2 things
Predicting salary of student(regression problem) - increase or so in salary

Accuracy score for Decision Tree (after dropping salary) = 0.8769230769230769

Classification report for the y-test and y-pred (low numbers)
          precision         recall   f1-score   support

           0       0.74      0.61      0.67        23
           1       0.80      0.88      0.84        42

    accuracy                           0.78        65
   macro avg       0.77      0.74      0.75        65
weighted avg       0.78      0.78      0.78        65

Random Forest Test for Accuracy score = 0.9384615384615385
Classification report for the y-test and y-pred (higher numbers)
             precision       recall  f1-score   support

           0       0.80      0.89      0.84        18
           1       0.96      0.91      0.93        47

    accuracy                           0.91        65
   macro avg       0.88      0.90      0.89        65
weighted avg       0.91      0.91      0.91        65

Tree based algorithms can be used to compute feature importance...

Based on the different test being compared to see the importance of the features
The different specialisation like features were all low therefore they do not seem to have much effect

Feature Scaling
Percentages are on scale 0-100
Categorical Features are on range 0-1 (By one hot encoding)
High Scale for Salary -> Salary is heavily skewed too -> SkLearn has RobustScaler which might work well here

With Logistic Regression accuracy score = 0.8461538461538461
Classification of the y-test and y-prediction
            precision       recall  f1-score   support

           0       0.88      0.70      0.78        20
           1       0.88      0.96      0.91        45

    accuracy                           0.88        65
   macro avg       0.88      0.83      0.85        65
weighted avg       0.88      0.88      0.87        65

The logistic regression ran well, going to run another method to see the importance of the different features

When looking at the MDA as text: This was with the test data therefore you can see a generalization.
Explained as: feature importances

*** Feature importances, computed as a decrease in score when feature
values are permuted (i.e. become noise). This is also known as
permutation importance. ***

*** If feature importances are computed on the same data as used for training,
they don't reflect importance of features for generalization. Use a held-out
dataset if you want generalization feature importances. ***
    weight      feature
0.1046 ± 0.0597  x0
0.0646 ± 0.0123  x1
0.0492 ± 0.0529  x2
0.0246 ± 0.0314  x10
0.0185 ± 0.0123  x14
0.0123 ± 0.0123  x9
0.0123 ± 0.0359  x4
0.0092 ± 0.0151  x6
0.0062 ± 0.0151  x5
0.0031 ± 0.0230  x13
     0 ± 0.0000  x3
     0 ± 0.0000  x8
     0 ± 0.0000  x7
     0 ± 0.0000  x11
     0 ± 0.0000  x12
-0.0031 ± 0.0230  x16
-0.0031 ± 0.0123  x15

rom Feature Importance of Tree-based Algorithms and MDA we can conclude that:

Academic performance affects placement (All percentages had importance)
Work Experience Effects Placement (small)
Gender and Specialization in Commerce (in higher-secondary and undergraduate) also has effect on placements.

Prediction of Salary (Regression Analysis)
after removing the nulls from salary and the status column
   gender  ssc_p  hsc_p  hsc_s  ...  etest_p  specialisation  mba_p    salary
1        0  79.33  78.33    1.0  ...    86.50               1  66.28  200000.0
2        0  65.00  68.00    2.0  ...    75.00               1  57.80  250000.0
7        0  82.00  64.00    1.0  ...    67.00               1  62.14  252000.0
19       0  60.00  67.00    2.0  ...    50.48               1  77.89  236000.0
22       1  69.80  60.80    1.0  ...    55.53               0  68.81  360000.0

after processing this data next step is to remove the different features that aren't important
method to use is the backward stepwise selection which is to remove the least important feature one by one
till you reach the stopping rule or there's no more features

Determining the least significant variable/feature

the least significant variable can be determined by:
highest p-value
removing it reduces the R2 to the lowest value compared to the other features
removing it has the least increment in residuals-sum-of-squares(RSS)

Identifying and removing the outliers
39     411000.0
53     450000.0
77     500000.0
150    690000.0
163    500000.0
174    500000.0

From the plot there was a slight decrease in the performance in the features (Sequential Backward Elimination)

Top 5 highest R2 scores among the features
{'feature_idx': (0, 2, 4, 5, 9), 'cv_scores': array([-0.40680187, -0.1276183 , -0.84695182,  0.41371216, -0.2412455 ,
       -0.5213583 ,  0.14384492,  0.15512385,  0.4403837 , -0.20121797]), 'avg_score': -0.11921291270740975,
       'feature_names': ('0', '2', '4', '5', '9'),
       'ci_bound': 0.29120952227282537,
       'std_dev': 0.3920889248208412,
       'std_err': 0.13069630827361373}
Most Significant Features:
gender
hsc_p
degree_p
degree_t
mba_p

FROM THE EXAMPLE THE PERSONS RESULTS ARE SOMEWHAT A BIT DIFFERENT
Most Significant 5 Features:
gender
hsc_s
degree_t
etest_p
mba_p

The r2 score and the mean absolute error of the top 5 highest r2 scoring features:
R2 Score: 0.19873988181788338
MAE: 28824.993060416797

Determining the Least Significant Variables by P-Values
If the base model gives 0.7 R2 score and the model without a feature gives 0.75 R2 score, we cannot conclude that
feature makes the difference, as the score may vary in another trial; in 10 trials the R2 score might change
in +/- 0.05. However, if model only varies in +/- 0.01, we can then say that removing a feature made the model better.

Our null hypothesis is that there is no difference between the two samples of R2 scores.
(UNDERSTANDING THAT WE BELIEVE THAT WITH ALL FEATURES AND WITH FEATURES REMOVED THEN THE r2 SCORE WILL BE NOT REALLY DIFFERENT)

P-value is the probability that you would arrive at the same results as the null hypothesis. One of the most
commonly used p-value is 0.05. If the calculated p-value turns out to be less than 0.05, the null hypothesis
is considered to be false, or nullified (hence the name null hypothesis). And if the value is greater than 0.05,
the null hypothesis is considered to be true.
(SO IF THE P-VALUE IS MORE THAN 0.05 THEN REMOVING THE FEATURES OR NOT, MAKES NOT MUCH DIFFERENCES BUT IF LESS THAN 0.05
REMOVING THE FEATURES OR NOT MAKES A DIFFERENCE)

For a feature, a small p-value indicates that it is unlikely we will observe a relationship between the
predictor (feature) and response (salary in our case) variables due to chance.
(SO IF THE P-VALUE IS LESS THAN 0.05 THEN THE FEATURES DOESN'T AFFECT THE SALARY)

Thus, we start with all features. We compute the P-values. We eliminate
feature with highest p-value until p-values of all features reach below threshold: 0.05.
(SO ALL THE ONES REMOVED IS BELIEVED TO AFFECT THE SALARY AND IT'S NOT BY CHANCE IF NONE REMOVED THEN THEY ARE LIKELY TO
BE A COINCIDENCE THIS DEPENDS ON THE INITIAL P-VALUE)

Added a constant 1 to be the intercept before doing the line regression to all the features
  const  gender     ssc_p     hsc_p  hsc_s  degree_p  degree_t  workex  \
0    1.0     0.0  0.750743  0.586729    0.0  0.613714       0.5     1.0
1    1.0     0.0  0.396040  0.366332    1.0  0.228571       0.0     0.0
2    1.0     0.0  0.816832  0.280990    0.0  0.285714       0.5     1.0
3    1.0     0.0  0.272277  0.344997    1.0  0.400000       0.0     1.0
4    1.0     1.0  0.514851  0.212716    0.0  0.463714       0.5     0.0

    etest_p  specialisation     mba_p
0  0.757997             1.0  0.538920
1  0.515993             1.0  0.202145
2  0.347643             1.0  0.374504
3  0.000000             1.0  1.000000
4  0.106271             0.0  0.639396

Finding the p-value of all the features

  OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.233
Model:                            OLS   Adj. R-squared:                  0.079
Method:                 Least Squares   F-statistic:                     1.517
Date:                Sat, 26 Feb 2022   Prob (F-statistic):              0.161
Time:                        14:45:52   Log-Likelihood:                -728.11
No. Observations:                  61   AIC:                             1478.
Df Residuals:                      50   BIC:                             1501.
Df Model:                          10
Covariance Type:            nonrobust
==================================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------
const            2.85e+05   2.13e+04     13.399      0.000    2.42e+05    3.28e+05
gender         -1.832e+04   1.21e+04     -1.511      0.137   -4.27e+04    6028.488
ssc_p          -1.512e+04    2.8e+04     -0.540      0.591   -7.13e+04    4.11e+04
hsc_p          -3.203e+04   3.34e+04     -0.960      0.342   -9.91e+04     3.5e+04
hsc_s          -2.961e+04   2.16e+04     -1.374      0.176   -7.29e+04    1.37e+04
degree_p       -6.758e+04   3.62e+04     -1.868      0.068    -1.4e+05    5084.307
degree_t        4.021e+04   1.83e+04      2.200      0.032    3498.976    7.69e+04
workex         -3896.7946   1.16e+04     -0.336      0.739   -2.72e+04    1.94e+04
etest_p         5215.7441    2.1e+04      0.248      0.805   -3.71e+04    4.75e+04
specialisation  4483.7135   1.19e+04      0.376      0.709   -1.95e+04    2.85e+04
mba_p           4.902e+04   2.82e+04      1.740      0.088   -7563.154    1.06e+05
==============================================================================
Omnibus:                        6.850   Durbin-Watson:                   1.966
Prob(Omnibus):                  0.033   Jarque-Bera (JB):                6.243
Skew:                           0.769   Prob(JB):                       0.0441
Kurtosis:                       3.303   Cond. No.                         14.2
==============================================================================

Highest p-value is etest_p (closest to 1) will remove this feature which has 0.805

Without etest_p

OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.232
Model:                            OLS   Adj. R-squared:                  0.096
Method:                 Least Squares   F-statistic:                     1.710
Date:                Sat, 26 Feb 2022   Prob (F-statistic):              0.111
Time:                        15:03:17   Log-Likelihood:                -728.14
No. Observations:                  61   AIC:                             1476.
Df Residuals:                      51   BIC:                             1497.
Df Model:                           9
Covariance Type:            nonrobust
==================================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------
const           2.858e+05   2.08e+04     13.714      0.000    2.44e+05    3.28e+05
gender         -1.868e+04   1.19e+04     -1.567      0.123   -4.26e+04    5246.690
ssc_p          -1.445e+04   2.76e+04     -0.524      0.603   -6.98e+04    4.09e+04
hsc_p          -3.121e+04   3.29e+04     -0.949      0.347   -9.73e+04    3.48e+04
hsc_s          -3.048e+04   2.11e+04     -1.447      0.154   -7.28e+04    1.18e+04
degree_p       -6.758e+04   3.58e+04     -1.885      0.065    -1.4e+05    4381.023
degree_t        4.063e+04    1.8e+04      2.253      0.029    4429.775    7.68e+04
workex         -3902.8425   1.15e+04     -0.339      0.736    -2.7e+04    1.92e+04
specialisation  5419.6803   1.12e+04      0.483      0.631   -1.71e+04    2.79e+04
mba_p           5.031e+04   2.74e+04      1.834      0.073   -4769.825    1.05e+05
==============================================================================
Omnibus:                        6.393   Durbin-Watson:                   1.967
Prob(Omnibus):                  0.041   Jarque-Bera (JB):                5.795
Skew:                           0.745   Prob(JB):                       0.0552
Kurtosis:                       3.249   Cond. No.                         13.6
==============================================================================

Highest p-value is workex (closest to 1) will remove this feature which has 0.736

Without workex

OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.230
Model:                            OLS   Adj. R-squared:                  0.112
Method:                 Least Squares   F-statistic:                     1.942
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0731
Time:                        15:18:06   Log-Likelihood:                -728.21
No. Observations:                  61   AIC:                             1474.
Df Residuals:                      52   BIC:                             1493.
Df Model:                           8
Covariance Type:            nonrobust
==================================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------
const           2.843e+05   2.02e+04     14.090      0.000    2.44e+05    3.25e+05
gender         -1.791e+04   1.16e+04     -1.544      0.129   -4.12e+04    5367.062
ssc_p          -1.473e+04   2.73e+04     -0.539      0.592   -6.96e+04    4.01e+04
hsc_p           -2.97e+04   3.23e+04     -0.919      0.362   -9.46e+04    3.52e+04
hsc_s          -3.023e+04   2.09e+04     -1.448      0.154   -7.21e+04    1.17e+04
degree_p       -6.554e+04    3.5e+04     -1.871      0.067   -1.36e+05    4762.688
degree_t        3.934e+04   1.75e+04      2.251      0.029    4273.410    7.44e+04
specialisation  4815.3168    1.1e+04      0.438      0.663   -1.72e+04    2.69e+04
mba_p            4.78e+04   2.62e+04      1.825      0.074   -4766.057       1e+05
==============================================================================
Omnibus:                        6.430   Durbin-Watson:                   1.978
Prob(Omnibus):                  0.040   Jarque-Bera (JB):                5.813
Skew:                           0.744   Prob(JB):                       0.0547
Kurtosis:                       3.266   Cond. No.                         12.6
==============================================================================
Highest p-value is specialisation (closest to 1) will remove this feature which has 0.663

Without specialisation
OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.227
Model:                            OLS   Adj. R-squared:                  0.125
Method:                 Least Squares   F-statistic:                     2.226
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0464
Time:                        15:23:48   Log-Likelihood:                -728.32
No. Observations:                  61   AIC:                             1473.
Df Residuals:                      53   BIC:                             1490.
Df Model:                           7
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2.864e+05   1.94e+04     14.740      0.000    2.47e+05    3.25e+05
gender     -1.906e+04   1.12e+04     -1.699      0.095   -4.16e+04    3435.599
ssc_p      -1.397e+04   2.71e+04     -0.516      0.608   -6.83e+04    4.03e+04
hsc_p      -3.041e+04    3.2e+04     -0.949      0.347   -9.47e+04    3.38e+04
hsc_s       -2.83e+04   2.03e+04     -1.398      0.168   -6.89e+04    1.23e+04
degree_p   -6.419e+04   3.46e+04     -1.853      0.069   -1.34e+05    5276.984
degree_t    3.908e+04   1.73e+04      2.255      0.028    4324.012    7.38e+04
mba_p       4.869e+04   2.59e+04      1.878      0.066   -3305.896    1.01e+05
==============================================================================
Omnibus:                        5.799   Durbin-Watson:                   1.968
Prob(Omnibus):                  0.055   Jarque-Bera (JB):                5.203
Skew:                           0.709   Prob(JB):                       0.0742
Kurtosis:                       3.197   Cond. No.                         11.6
==============================================================================
Highest p-value is ssc_p (closest to 1) will remove this feature which has 0.608

Without ssc_p
OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.223
Model:                            OLS   Adj. R-squared:                  0.137
Method:                 Least Squares   F-statistic:                     2.588
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0281
Time:                        15:29:15   Log-Likelihood:                -728.48
No. Observations:                  61   AIC:                             1471.
Df Residuals:                      54   BIC:                             1486.
Df Model:                           6
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2.821e+05   1.75e+04     16.152      0.000    2.47e+05    3.17e+05
gender      -2.03e+04   1.09e+04     -1.866      0.068   -4.21e+04    1514.869
hsc_p      -3.512e+04   3.05e+04     -1.151      0.255   -9.63e+04     2.6e+04
hsc_s      -2.459e+04   1.88e+04     -1.308      0.196   -6.23e+04    1.31e+04
degree_p   -6.536e+04   3.43e+04     -1.904      0.062   -1.34e+05    3450.533
degree_t    3.809e+04   1.71e+04      2.227      0.030    3796.517    7.24e+04
mba_p       4.492e+04   2.47e+04      1.818      0.075   -4606.570    9.44e+04
==============================================================================
Omnibus:                        6.735   Durbin-Watson:                   1.961
Prob(Omnibus):                  0.034   Jarque-Bera (JB):                6.103
Skew:                           0.759   Prob(JB):                       0.0473
Kurtosis:                       3.308   Cond. No.                         10.5
==============================================================================
Highest p-value is hsc_p (closest to 1) will remove this feature which has 0.255

Without hsc_p
OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.204
Model:                            OLS   Adj. R-squared:                  0.132
Method:                 Least Squares   F-statistic:                     2.824
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0243
Time:                        15:33:53   Log-Likelihood:                -729.22
No. Observations:                  61   AIC:                             1470.
Df Residuals:                      55   BIC:                             1483.
Df Model:                           5
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2.717e+05    1.5e+04     18.151      0.000    2.42e+05    3.02e+05
gender     -1.718e+04   1.06e+04     -1.626      0.110   -3.84e+04    3999.217
hsc_s      -2.738e+04   1.87e+04     -1.464      0.149   -6.49e+04    1.01e+04
degree_p   -6.478e+04   3.44e+04     -1.882      0.065   -1.34e+05    4199.931
degree_t     3.43e+04   1.68e+04      2.037      0.046     561.607     6.8e+04
mba_p       4.044e+04   2.45e+04      1.653      0.104   -8590.569    8.95e+04
==============================================================================
Omnibus:                        4.766   Durbin-Watson:                   2.025
Prob(Omnibus):                  0.092   Jarque-Bera (JB):                4.361
Skew:                           0.655   Prob(JB):                        0.113
Kurtosis:                       3.008   Cond. No.                         10.0
==============================================================================
Highest p-value is hsc_s (closest to 1) will remove this feature which has 0.149

Without hsc_p
OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.173
Model:                            OLS   Adj. R-squared:                  0.114
Method:                 Least Squares   F-statistic:                     2.934
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0284
Time:                        15:44:51   Log-Likelihood:                -730.38
No. Observations:                  61   AIC:                             1471.
Df Residuals:                      56   BIC:                             1481.
Df Model:                           4
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        2.66e+05   1.46e+04     18.217      0.000    2.37e+05    2.95e+05
gender     -1.994e+04   1.05e+04     -1.898      0.063    -4.1e+04    1109.269
degree_p   -4.603e+04   3.23e+04     -1.426      0.159   -1.11e+05    1.86e+04
degree_t    3.647e+04   1.69e+04      2.153      0.036    2532.483    7.04e+04
mba_p       3.109e+04   2.39e+04      1.303      0.198   -1.67e+04    7.89e+04
==============================================================================
Omnibus:                        6.700   Durbin-Watson:                   1.917
Prob(Omnibus):                  0.035   Jarque-Bera (JB):                6.224
Skew:                           0.776   Prob(JB):                       0.0445
Kurtosis:                       3.199   Cond. No.                         9.00
==============================================================================
Example was able to stop because the rest were closer to 0.05 than what I have at the moment
mba_p is still really high for mine therefore going to have to drop as well.

Without mba_p
OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.148
Model:                            OLS   Adj. R-squared:                  0.103
Method:                 Least Squares   F-statistic:                     3.306
Date:                Sat, 26 Feb 2022   Prob (F-statistic):             0.0265
Time:                        15:48:16   Log-Likelihood:                -731.29
No. Observations:                  61   AIC:                             1471.
Df Residuals:                      57   BIC:                             1479.
Df Model:                           3
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2.701e+05   1.43e+04     18.837      0.000    2.41e+05    2.99e+05
gender     -1.847e+04   1.05e+04     -1.757      0.084   -3.95e+04    2576.353
degree_p    -2.84e+04   2.95e+04     -0.963      0.339   -8.74e+04    3.06e+04
degree_t    3.985e+04   1.68e+04      2.366      0.021    6126.338    7.36e+04
==============================================================================
Omnibus:                        8.203   Durbin-Watson:                   1.847
Prob(Omnibus):                  0.017   Jarque-Bera (JB):                7.917
Skew:                           0.872   Prob(JB):                       0.0191
Kurtosis:                       3.272   Cond. No.                         7.58
==============================================================================

degree_p was still pretty high when mba_p was removed but, I thought this was a good place to stop
for we were looking for the top 5 features that affects the salary

The top 5 features that affects salary:
    -gender
    -degree_p
    -degree_t
    -mba_p
    -hsc_s
 This was the same as the Sequential Feature Selection with the mlxtend for looking for most significant
 based on the R2 score
Most Significant Features based on R2 Score :
    -gender
    -hsc_p
    -degree_p
    -degree_t
    -mba_p

 only difference is the hsc_p and hsc_s through this method hsc_p was always higher than hsc_s therefore it was removed earlier
 however in the R2 score test said that the top sig. would be the hsc_p. Other than that its mostly the same but different
 from the example for some reason.

 This might be due to:
 Machine learning algorithms are non-deterministic algorithms, where you will have different +- outputs each time
 you run the algorithm on the same inputs. you may control this if you have the option to disable the randomness
 selection of data, moreover, tuning the learning rate variable will enhance the output taking into consideration
 your data-set size.

